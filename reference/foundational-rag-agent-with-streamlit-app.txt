Directory structure:
└── foundational-rag-agent/
    ├── README.md
    ├── PLANNING.md
    ├── prompt.txt
    ├── rag-example.sql
    ├── requirements.txt
    ├── streamlit_ui_example.py
    ├── TASK.md
    ├── .env.example
    ├── .gitignore
    ├── agent/
    │   ├── __init__.py
    │   ├── agent.py
    │   ├── prompts.py
    │   └── tools.py
    ├── database/
    │   ├── __init__.py
    │   ├── setup.py
    │   └── setup_db.py
    ├── document_processing/
    │   ├── __init__.py
    │   ├── chunker.py
    │   ├── embeddings.py
    │   ├── ingestion.py
    │   └── processors.py
    ├── tests/
    │   ├── test_agent.py
    │   ├── test_agent_tools.py
    │   ├── test_chunker.py
    │   └── test_processors.py
    ├── ui/
    │   └── app.py
    └── .windsurf/
        └── rules/
            └── primary-guide.md

================================================
FILE: foundational-rag-agent/README.md
================================================
# RAG AI Agent with Pydantic AI and Supabase

A simple Retrieval-Augmented Generation (RAG) AI agent using Pydantic AI and Supabase with pgvector for document storage and retrieval.

## Features

- Document ingestion pipeline for TXT and PDF files
- Vector embeddings using OpenAI
- Document storage in Supabase with pgvector
- Pydantic AI agent with knowledge base search capabilities
- Streamlit UI for document uploads and agent interaction

## Project Structure

```
foundational-rag-agent/
├── database/
│   └── setup.py          # Database setup and connection utilities
├── document_processing/
│   ├── __init__.py
│   ├── chunker.py        # Text chunking functionality
│   ├── embeddings.py     # Embeddings generation with OpenAI
│   ├── ingestion.py      # Document ingestion pipeline
│   └── processors.py     # TXT and PDF processing
├── agent/
│   ├── __init__.py
│   ├── agent.py          # Main agent definition
│   ├── prompts.py        # System prompts
│   └── tools.py          # Knowledge base search tool
├── ui/
│   └── app.py            # Streamlit application
├── tests/
│   ├── test_chunker.py
│   ├── test_embeddings.py
│   ├── test_ingestion.py
│   ├── test_processors.py
│   └── test_agent.py
├── .env.example          # Example environment variables
├── requirements.txt      # Project dependencies
├── PLANNING.md           # Project planning document
├── TASK.md               # Task tracking
└── README.md             # Project documentation
```

## Setup

1. Clone the repository
2. Create a virtual environment:
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
4. Copy `.env.example` to `.env` and fill in your API keys and configuration
5. Run the Streamlit application:
   ```
   streamlit run ui/app.py
   ```
6. Run the SQL in `rag-example.sql` to create the table and matching function for RAG

## Usage

1. Upload documents (TXT or PDF) through the Streamlit UI
2. Ask questions to the AI agent
3. View responses with source attribution

## Dependencies

- Python 3.11+
- Pydantic AI
- Supabase
- OpenAI
- PyPDF2
- Streamlit

## License

MIT



================================================
FILE: foundational-rag-agent/PLANNING.md
================================================
# Project Planning: RAG AI Agent with Pydantic AI and Supabase

## Project Overview
We're building a simple Retrieval-Augmented Generation (RAG) AI agent using the Pydantic AI library. The agent will have access to a knowledge base stored in Supabase, allowing it to retrieve relevant information to answer user queries. The system will include functionality to ingest local text and PDF files, process them, and store them in Supabase for later retrieval.

## Architecture

### Core Components:
1. **Document Ingestion Pipeline**
   - Accept local files (TXT, PDF)
   - Simple text processing and chunking (without external libraries)
   - Generate embeddings using OpenAI embeddings API
   - Store documents and embeddings in Supabase

2. **Supabase Database**
   - Store document chunks and their embeddings with pgvector
   - Support semantic search for efficient retrieval
   - Tables will be created and managed via Supabase MCP server

3. **Pydantic AI Agent**
   - Define a tool to query the knowledge base
   - Use OpenAI models for generating responses
   - Integrate knowledge base search results into responses

4. **Streamlit User Interface**
   - Interface for uploading documents
   - Interface for querying the AI agent
   - Display agent responses

### Technology Stack:
- **Language**: Python 3.11+
- **AI Framework**: Pydantic AI for agent implementation
- **Database**: Supabase with pgvector extension
- **Embeddings**: OpenAI embeddings API
- **LLM Provider**: OpenAI (GPT-4.1 mini or similar)
- **UI**: Streamlit
- **Document Processing**: Simple text processing with PyPDF2 for PDF extraction

## Development Process

The development will follow a task-based approach where each component will be implemented sequentially. We should:

1. Start by setting up the project structure
2. Create database tables using Supabase MCP server
3. Implement simple document ingestion pipeline
4. Create the Pydantic AI agent with knowledge base search tool
5. Develop Streamlit UI
6. Connect all components and ensure they work together
7. Test the complete system

## Design Principles

1. **Modularity**: Keep components decoupled for easier maintenance
2. **Simplicity**: Focus on making the system easy to understand and modify
3. **Performance**: Optimize for response time in knowledge retrieval
4. **User Experience**: Make the Streamlit interface intuitive

## Environment Configuration

Create a `.env.example` file with the following variables:
- `OPENAI_API_KEY`: For embeddings and LLM
- `OPENAI_MODEL`: e.g., "gpt-4.1-mini" or other models
- `SUPABASE_URL`: URL for Supabase instance
- `SUPABASE_KEY`: API key for Supabase

This file will serve as a template for users to create their own `.env` file.

## Expected Output

A functional RAG system where users can:
- Upload local text or PDF documents to build a knowledge base
- Ask questions to the AI agent
- Receive responses that incorporate information from the knowledge base

## Notes

When implementing this project, make sure to:
- Mark tasks complete in the task.md file as you finish them
- Use the Supabase MCP server to create and manage database tables
- Build a simple document ingestion pipeline without complex libraries
- Focus on creating a working Pydantic AI agent that can effectively retrieve and use information from the knowledge base
- Create a clean, intuitive Streamlit interface


================================================
FILE: foundational-rag-agent/prompt.txt
================================================
I'd like to build a RAG AI agent with Pydantic AI and Supabase, using the following MCP servers:

Be sure to review the planning and task files.
This project should create a simple RAG system with:

A document ingestion pipeline that:

Accepts local TXT and PDF files
Uses a simple chunking approach
Generates embeddings using OpenAI
Stores documents and vectors in Supabase with pgvector


A Pydantic AI agent that:

Has a tool for knowledge base search
Uses OpenAI models for response generation
Integrates retrieved contexts into responses


A Streamlit UI that:

Allows document uploads
Provides a clean interface for querying the agent
Displays responses with source attribution
Use @streamlit_ui_example.py to see exactly how to integrate Streamlit with a Pydantic AI agent.


Use the Supabase MCP server to create the necessary database tables with the pgvector extension enabled. For document processing, keep it simple using PyPDF2 for PDFs rather than complex document processing libraries.

Use the Crawl4AI RAG MCP server that already has the Pydantic AI and Supabase Python documentation available. So just perform RAG queries whenever necessary. Also use the Brave MCP server to search the web for supplemental docs/examples to aid in creating the agent.


================================================
FILE: foundational-rag-agent/rag-example.sql
================================================
-- Enable the pgvector extension
create extension if not exists vector;

-- Create the documentation chunks table
create table rag_pages (
    id bigserial primary key,
    url varchar not null,
    chunk_number integer not null,
    content text not null,  -- Added content column
    metadata jsonb not null default '{}'::jsonb,  -- Added metadata column
    embedding vector(1536),  -- OpenAI embeddings are 1536 dimensions
    created_at timestamp with time zone default timezone('utc'::text, now()) not null,
    
    -- Add a unique constraint to prevent duplicate chunks for the same URL
    unique(url, chunk_number)
);

-- Create an index for better vector similarity search performance
create index on rag_pages using ivfflat (embedding vector_cosine_ops);

-- Create an index on metadata for faster filtering
create index idx_rag_pages_metadata on rag_pages using gin (metadata);

CREATE INDEX idx_rag_pages_source ON rag_pages ((metadata->>'source'));

-- Create a function to search for documentation chunks
create or replace function match_rag_pages (
  query_embedding vector(1536),
  match_count int default 10,
  filter jsonb DEFAULT '{}'::jsonb
) returns table (
  id bigint,
  url varchar,
  chunk_number integer,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    url,
    chunk_number,
    content,
    metadata,
    1 - (rag_pages.embedding <=> query_embedding) as similarity
  from rag_pages
  where metadata @> filter
  order by rag_pages.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Enable RLS on the table
alter table rag_pages enable row level security;

-- Create a policy that allows anyone to read
create policy "Allow public read access"
  on rag_pages
  for select
  to public
  using (true);


================================================
FILE: foundational-rag-agent/requirements.txt
================================================
pydantic-ai>=0.1.12
supabase>=2.0.0
openai>=1.0.0
PyPDF2>=3.0.0
streamlit>=1.30.0
python-dotenv>=1.0.0
numpy>=1.24.0
pytest>=7.0.0
pytest-asyncio>=0.21.0



================================================
FILE: foundational-rag-agent/streamlit_ui_example.py
================================================
from pydantic_ai import Agent
from httpx import AsyncClient
import streamlit as st
import asyncio
import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from Basic_Pydantic_AI_Agent.src import agent, AgentDeps

# Import all the message part classes from Pydantic AI
from pydantic_ai.messages import ModelRequest, ModelResponse, PartDeltaEvent, PartStartEvent, TextPartDelta

def display_message_part(part):
    """
    Display a single part of a message in the Streamlit UI.
    Customize how you display system prompts, user prompts,
    tool calls, tool returns, etc.
    """
    # User messages
    if part.part_kind == 'user-prompt' and part.content:
        with st.chat_message("user"):
            st.markdown(part.content)
    # AI messages
    elif part.part_kind == 'text' and part.content:
        with st.chat_message("assistant"):
            st.markdown(part.content)             

async def run_agent_with_streaming(user_input):
    async with AsyncClient() as http_client:
        agent_deps = AgentDeps(
            http_client=http_client,
            brave_api_key=os.getenv("BRAVE_API_KEY", ""),
            searxng_base_url=os.getenv("SEARXNG_BASE_URL", "")
        )   

        async with agent.iter(user_input, deps=agent_deps, message_history=st.session_state.messages) as run:
            async for node in run:
                if Agent.is_model_request_node(node):
                    # A model request node => We can stream tokens from the model's request
                    async with node.stream(run.ctx) as request_stream:
                        async for event in request_stream:
                            if isinstance(event, PartStartEvent) and event.part.part_kind == 'text':
                                    yield event.part.content
                            elif isinstance(event, PartDeltaEvent) and isinstance(event.delta, TextPartDelta):
                                    delta = event.delta.content_delta
                                    yield delta         

    # Add the new messages to the chat history (including tool calls and responses)
    st.session_state.messages.extend(run.result.new_messages())       


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~ Main Function with UI Creation ~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

async def main():
    st.title("Pydantic AI Agent")
    
    # Initialize chat history in session state if not present
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display all messages from the conversation so far
    # Each message is either a ModelRequest or ModelResponse.
    # We iterate over their parts to decide how to display them.
    for msg in st.session_state.messages:
        if isinstance(msg, ModelRequest) or isinstance(msg, ModelResponse):
            for part in msg.parts:
                display_message_part(part)

    # Chat input for the user
    user_input = st.chat_input("What do you want to do today?")

    if user_input:
        # Display user prompt in the UI
        with st.chat_message("user"):
            st.markdown(user_input)

        # Display the assistant's partial response while streaming
        with st.chat_message("assistant"):
            # Create a placeholder for the streaming text
            message_placeholder = st.empty()
            full_response = ""
            
            # Properly consume the async generator with async for
            generator = run_agent_with_streaming(user_input)
            async for message in generator:
                full_response += message
                message_placeholder.markdown(full_response + "▌")
            
            # Final response without the cursor
            message_placeholder.markdown(full_response)


if __name__ == "__main__":
    asyncio.run(main())


================================================
FILE: foundational-rag-agent/TASK.md
================================================
# RAG AI Agent Tasks

## Project Setup
- [x] Review PLANNING.md
- [x] Create project structure
- [x] Set up environment configuration (.env.example)

## Database Setup
- [x] Create Supabase tables with pgvector extension
- [x] Set up vector search functionality

## Document Ingestion Pipeline
- [x] Implement TXT file processing
- [x] Implement PDF file processing with PyPDF2
- [x] Create text chunking functionality
- [x] Implement OpenAI embeddings generation
- [x] Create document storage in Supabase

## Pydantic AI Agent
- [x] Create knowledge base search tool
- [x] Implement agent with OpenAI model integration
- [x] Set up context integration for responses

## Streamlit UI
- [x] Create document upload interface
- [x] Implement agent query interface
- [x] Add source attribution display
- [x] Connect UI to agent and document pipeline

## Testing
- [x] Create unit tests for document processing
- [x] Create unit tests for knowledge base search
- [x] Create unit tests for agent functionality

## Documentation
- [x] Update README.md with setup and usage instructions

## Discovered During Work
- [ ] Add support for more document types (e.g., DOCX, HTML)
- [ ] Implement metadata filtering in the UI
- [ ] Add visualization of vector embeddings
- [ ] Create a CLI interface for batch document processing



================================================
FILE: foundational-rag-agent/.env.example
================================================
# OpenAI API configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4.1-mini  # Or other OpenAI model

# Supabase configuration
SUPABASE_URL=your_supabase_url_here
SUPABASE_KEY=your_supabase_key_here

# Embedding configuration
EMBEDDING_MODEL=text-embedding-3-small  # OpenAI embedding model

# Application settings
CHUNK_SIZE=1000  # Size of text chunks for embedding
CHUNK_OVERLAP=200  # Overlap between chunks
MAX_RESULTS=5  # Maximum number of results to return from vector search



================================================
FILE: foundational-rag-agent/.gitignore
================================================
.env
__pycache__
venv


================================================
FILE: foundational-rag-agent/agent/__init__.py
================================================
"""
Agent package for RAG AI agent.
"""
from agent.agent import RAGAgent, AgentDeps, agent
from agent.tools import KnowledgeBaseSearch, KnowledgeBaseSearchParams, KnowledgeBaseSearchResult

__all__ = [
    'RAGAgent',
    'AgentDeps',
    'agent',
    'KnowledgeBaseSearch',
    'KnowledgeBaseSearchParams',
    'KnowledgeBaseSearchResult'
]



================================================
FILE: foundational-rag-agent/agent/agent.py
================================================
"""
Main agent definition for the RAG AI agent.
"""
import os
import sys
from typing import List, Dict, Any, Optional, TypedDict
from pydantic_ai import Agent
from pydantic_ai.tools import Tool
from dotenv import load_dotenv
from pathlib import Path

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent.tools import KnowledgeBaseSearch, KnowledgeBaseSearchParams, KnowledgeBaseSearchResult
from agent.prompts import RAG_SYSTEM_PROMPT

# Load environment variables from the project root .env file
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'

# Force override of existing environment variables
load_dotenv(dotenv_path, override=True)

class AgentDeps(TypedDict, total=False):
    """
    Dependencies for the RAG agent.
    """
    kb_search: KnowledgeBaseSearch


class RAGAgent:
    """
    RAG AI agent with knowledge base search capabilities.
    
    Args:
        model: OpenAI model to use. Defaults to OPENAI_MODEL env var.
        api_key: OpenAI API key. Defaults to OPENAI_API_KEY env var.
        kb_search: KnowledgeBaseSearch instance for searching the knowledge base.
    """
    
    def __init__(
        self,
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        kb_search: Optional[KnowledgeBaseSearch] = None
    ):
        """
        Initialize the RAG agent.
        """
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4.1-mini")
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        
        if not self.api_key:
            raise ValueError("OpenAI API key must be provided either as an argument or environment variable.")
        
        # Initialize the knowledge base search tool
        self.kb_search = kb_search or KnowledgeBaseSearch()
        
        # Create the search tool
        self.search_tool = Tool(self.kb_search.search)
        
        # Initialize the Pydantic AI agent
        self.agent = Agent(
            f"openai:{self.model}",
            system_prompt=RAG_SYSTEM_PROMPT,
            tools=[self.search_tool]
        )
    
    async def query(
        self, 
        question: str, 
        max_results: int = 5,
        source_filter: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Query the RAG agent with a question.
        
        Args:
            question: The question to ask
            max_results: Maximum number of knowledge base results to retrieve
            source_filter: Optional filter to search only within a specific source
            
        Returns:
            Dictionary containing the agent's response and the knowledge base search results
        """
        # Create dependencies for the agent
        deps = AgentDeps(kb_search=self.kb_search)
        
        # Run the agent with the question
        result = await self.agent.run(
            question,
            deps=deps
        )
        
        # Get the agent's response
        response = result.output
        
        # Get the knowledge base search results from the tool calls
        kb_results = []
        for tool_call in result.tool_calls:
            if tool_call.tool.name == "search":
                kb_results = tool_call.result
        
        return {
            "response": response,
            "kb_results": kb_results
        }
    
    async def get_available_sources(self) -> List[str]:
        """
        Get a list of all available sources in the knowledge base.
        
        Returns:
            List of source identifiers
        """
        return await self.kb_search.get_available_sources()


# Create a singleton instance for easy import
agent = RAGAgent()



================================================
FILE: foundational-rag-agent/agent/prompts.py
================================================
"""
System prompts for the RAG AI agent.
"""

# System prompt for the RAG agent
RAG_SYSTEM_PROMPT = """You are a helpful AI assistant with access to a knowledge base.
When answering questions, you should:

1. Use the knowledge base search results when they are relevant to the question.
2. Cite your sources by mentioning the document name when you use information from the knowledge base.
3. If the knowledge base doesn't contain relevant information, use your general knowledge to answer.
4. If you don't know the answer, be honest and say so.
5. Keep your answers concise and to the point.
6. Format your responses using markdown for better readability.

Remember to always provide accurate information and acknowledge when information comes from the knowledge base.
"""



================================================
FILE: foundational-rag-agent/agent/tools.py
================================================
"""
Knowledge base search tool for the RAG AI agent.
"""
import os
import sys
from typing import Dict, List, Any, Optional
from pydantic import BaseModel, Field

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.setup import SupabaseClient
from document_processing.embeddings import EmbeddingGenerator

class KnowledgeBaseSearchParams(BaseModel):
    """
    Parameters for the knowledge base search tool.
    """
    query: str = Field(..., description="The search query to find relevant information in the knowledge base")
    max_results: int = Field(5, description="Maximum number of results to return (default: 5)")
    source_filter: Optional[str] = Field(None, description="Optional filter to search only within a specific source")


class KnowledgeBaseSearchResult(BaseModel):
    """
    Result from the knowledge base search.
    """
    content: str = Field(..., description="Content of the document chunk")
    source: str = Field(..., description="Source of the document chunk")
    source_type: str = Field(..., description="Type of source (e.g., 'pdf', 'txt')")
    similarity: float = Field(..., description="Similarity score between the query and the document")
    metadata: Dict[str, Any] = Field(..., description="Additional metadata about the document")


class KnowledgeBaseSearch:
    """
    Tool for searching the knowledge base using vector similarity.
    """
    
    def __init__(
        self,
        supabase_client: Optional[SupabaseClient] = None,
        embedding_generator: Optional[EmbeddingGenerator] = None
    ):
        """
        Initialize the knowledge base search tool.
        
        Args:
            supabase_client: SupabaseClient instance for database operations
            embedding_generator: EmbeddingGenerator instance for creating embeddings
        """
        self.supabase_client = supabase_client or SupabaseClient()
        self.embedding_generator = embedding_generator or EmbeddingGenerator()
    
    async def search(self, params: KnowledgeBaseSearchParams) -> List[KnowledgeBaseSearchResult]:
        """
        Search the knowledge base for relevant information.
        
        Args:
            params: Search parameters
            
        Returns:
            List of search results
        """
        # Generate embedding for the query
        query_embedding = self.embedding_generator.embed_text(params.query)
        
        # Prepare filter metadata if source filter is provided
        filter_metadata = None
        if params.source_filter:
            filter_metadata = {"source": params.source_filter}
        
        # Search for documents
        results = self.supabase_client.search_documents(
            query_embedding=query_embedding,
            match_count=params.max_results,
            filter_metadata=filter_metadata
        )
        
        # Convert results to KnowledgeBaseSearchResult objects
        search_results = []
        for result in results:
            search_results.append(
                KnowledgeBaseSearchResult(
                    content=result["content"],
                    source=result["metadata"].get("source", "Unknown"),
                    source_type=result["metadata"].get("source_type", "Unknown"),
                    similarity=result["similarity"],
                    metadata=result["metadata"]
                )
            )
        
        return search_results
    
    async def get_available_sources(self) -> List[str]:
        """
        Get a list of all available sources in the knowledge base.
        
        Returns:
            List of source identifiers
        """
        return self.supabase_client.get_all_document_sources()



================================================
FILE: foundational-rag-agent/database/__init__.py
================================================
"""
Database package for RAG AI agent.
"""
from database.setup import SupabaseClient, setup_database_tables

__all__ = [
    'SupabaseClient',
    'setup_database_tables'
]



================================================
FILE: foundational-rag-agent/database/setup.py
================================================
"""
Database setup and connection utilities for Supabase with pgvector.
"""
import os
import json
from typing import Dict, List, Optional, Any
from dotenv import load_dotenv
from pathlib import Path
from supabase import create_client, Client

# Load environment variables from the project root .env file
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'

# Force override of existing environment variables
load_dotenv(dotenv_path, override=True)

class SupabaseClient:
    """
    Client for interacting with Supabase and pgvector.
    
    Args:
        supabase_url: URL for Supabase instance. Defaults to SUPABASE_URL env var.
        supabase_key: API key for Supabase. Defaults to SUPABASE_KEY env var.
    """
    
    def __init__(
        self, 
        supabase_url: Optional[str] = None, 
        supabase_key: Optional[str] = None
    ):
        """
        Initialize the Supabase client.
        """
        self.supabase_url = supabase_url or os.getenv("SUPABASE_URL")
        self.supabase_key = supabase_key or os.getenv("SUPABASE_KEY")
        
        if not self.supabase_url or not self.supabase_key:
            raise ValueError(
                "Supabase URL and key must be provided either as arguments or environment variables."
            )
        
        self.client = create_client(self.supabase_url, self.supabase_key)
    
    def store_document_chunk(
        self, 
        url: str, 
        chunk_number: int, 
        content: str, 
        embedding: List[float],
        metadata: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Store a document chunk with its embedding in Supabase.
        
        Args:
            url: Source URL or identifier for the document
            chunk_number: Chunk number within the document
            content: Text content of the chunk
            embedding: Vector embedding of the chunk
            metadata: Additional metadata about the chunk
            
        Returns:
            Dictionary containing the inserted record
        """
        if metadata is None:
            metadata = {}
            
        data = {
            "url": url,
            "chunk_number": chunk_number,
            "content": content,
            "embedding": embedding,
            "metadata": metadata
        }
        
        result = self.client.table("rag_pages").insert(data).execute()
        return result.data[0] if result.data else {}
    
    def search_documents(
        self, 
        query_embedding: List[float], 
        match_count: int = 5,
        filter_metadata: Dict[str, Any] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for document chunks by vector similarity.
        
        Args:
            query_embedding: Vector embedding of the query
            match_count: Maximum number of results to return
            filter_metadata: Optional metadata filter
            
        Returns:
            List of matching document chunks with similarity scores
        """
        # Prepare parameters for the RPC call
        params = {
            "query_embedding": query_embedding,
            "match_count": match_count
        }
        
        # Add filter if provided
        if filter_metadata:
            params["filter"] = filter_metadata
        
        # Call the match_rag_pages function
        result = self.client.rpc("match_rag_pages", params).execute()
        return result.data if result.data else []
    
    def get_document_by_id(self, doc_id: int) -> Dict[str, Any]:
        """
        Get a document chunk by its ID.
        
        Args:
            doc_id: ID of the document chunk
            
        Returns:
            Document chunk data
        """
        result = self.client.table("rag_pages").select("*").eq("id", doc_id).execute()
        return result.data[0] if result.data else {}
    
    def get_all_document_sources(self) -> List[str]:
        """
        Get a list of all unique document sources.
        
        Returns:
            List of unique source URLs/identifiers
        """
        result = self.client.table("rag_pages").select("url").execute()
        urls = set(item["url"] for item in result.data if result.data)
        return list(urls)
        
    def count_documents(self) -> int:
        """
        Count the total number of unique documents in the database.
        
        Returns:
            Number of unique documents (based on unique URLs)
        """
        return len(self.get_all_document_sources())


def setup_database_tables() -> None:
    """
    Set up the necessary database tables and functions for the RAG system.
    This should be run once to initialize the database.
    
    Note: This is typically done through the Supabase MCP server in production.
    """
    # This is a placeholder for the actual implementation
    # In a real application, you would use the Supabase MCP server to run the SQL
    pass



================================================
FILE: foundational-rag-agent/database/setup_db.py
================================================
"""
Script to set up the database tables in Supabase using the Supabase MCP server.
"""
import os
import sys
import asyncio
from pathlib import Path
from dotenv import load_dotenv

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Load environment variables from the project root .env file
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'

# Force override of existing environment variables
load_dotenv(dotenv_path, override=True)

# SQL for creating the database tables and functions
SQL_SETUP = """
-- Enable the pgvector extension
create extension if not exists vector;

-- Create the documentation chunks table
create table rag_pages (
    id bigserial primary key,
    url varchar not null,
    chunk_number integer not null,
    content text not null,
    metadata jsonb not null default '{}'::jsonb,
    embedding vector(1536),  -- OpenAI embeddings are 1536 dimensions
    created_at timestamp with time zone default timezone('utc'::text, now()) not null,
    
    -- Add a unique constraint to prevent duplicate chunks for the same URL
    unique(url, chunk_number)
);

-- Create an index for better vector similarity search performance
create index on rag_pages using ivfflat (embedding vector_cosine_ops);

-- Create an index on metadata for faster filtering
create index idx_rag_pages_metadata on rag_pages using gin (metadata);

-- Create an index on source for faster filtering
CREATE INDEX idx_rag_pages_source ON rag_pages ((metadata->>'source'));

-- Create a function to search for documentation chunks
create or replace function match_rag_pages (
  query_embedding vector(1536),
  match_count int default 10,
  filter jsonb DEFAULT '{}'::jsonb
) returns table (
  id bigint,
  url varchar,
  chunk_number integer,
  content text,
  metadata jsonb,
  similarity float
)
language plpgsql
as $$
#variable_conflict use_column
begin
  return query
  select
    id,
    url,
    chunk_number,
    content,
    metadata,
    1 - (rag_pages.embedding <=> query_embedding) as similarity
  from rag_pages
  where metadata @> filter
  order by rag_pages.embedding <=> query_embedding
  limit match_count;
end;
$$;

-- Enable RLS on the table
alter table rag_pages enable row level security;

-- Create a policy that allows anyone to read
create policy "Allow public read access"
  on rag_pages
  for select
  to public
  using (true);

-- Create a policy that allows anyone to insert
create policy "Allow public insert access"
  on rag_pages
  for insert
  to public
  with check (true);
"""

async def setup_database():
    """
    Set up the database tables and functions in Supabase.
    
    This function uses the Supabase MCP server to run the SQL setup script.
    """
    try:
        # In a real application, you would use the Supabase MCP server to run the SQL
        # For example:
        # result = await mcp2_apply_migration(name="rag_setup", query=SQL_SETUP)
        # print(f"Database setup completed: {result}")
        
        print("Database setup script generated.")
        print("To set up the database, use the Supabase MCP server to run the SQL script.")
        print("Example command:")
        print("mcp2_apply_migration(name=\"rag_setup\", query=SQL_SETUP)")
    except Exception as e:
        print(f"Error setting up database: {e}")


if __name__ == "__main__":
    asyncio.run(setup_database())



================================================
FILE: foundational-rag-agent/document_processing/__init__.py
================================================
"""
Document processing package for RAG AI agent.
"""
from document_processing.chunker import TextChunker
from document_processing.embeddings import EmbeddingGenerator
from document_processing.processors import DocumentProcessor, TxtProcessor, PdfProcessor, get_document_processor
from document_processing.ingestion import DocumentIngestionPipeline

__all__ = [
    'TextChunker',
    'EmbeddingGenerator',
    'DocumentProcessor',
    'TxtProcessor',
    'PdfProcessor',
    'get_document_processor',
    'DocumentIngestionPipeline'
]



================================================
FILE: foundational-rag-agent/document_processing/chunker.py
================================================
"""
Text chunking functionality for document processing.
"""
import os
from typing import List
from pathlib import Path

class TextChunker:
    """
    Simple text chunker that splits documents into manageable pieces.
    """
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Initialize the chunker with size and overlap settings.
        
        Args:
            chunk_size: Maximum size of each chunk in characters
            chunk_overlap: Number of characters to overlap between chunks
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = min(chunk_overlap, chunk_size // 2)  # Ensure overlap isn't too large
        
        print(f"Initialized TextChunker with size={chunk_size}, overlap={self.chunk_overlap}")
    
    def chunk_text(self, text: str) -> List[str]:
        """
        Split text into chunks using a simple sliding window approach.
        
        Args:
            text: The text to split into chunks
            
        Returns:
            List of text chunks
        """
        # Handle empty or very short text
        if not text or not text.strip():
            print("Warning: Empty text provided to chunker")
            return [""]
            
        if len(text) <= self.chunk_size:
            print(f"Text is only {len(text)} chars, returning as single chunk")
            return [text]
        
        # Simple sliding window chunking
        chunks = []
        step_size = self.chunk_size - self.chunk_overlap
        
        # Ensure step size is at least 100 characters to prevent infinite loops
        if step_size < 100:
            step_size = 100
            print(f"Warning: Adjusted step size to {step_size} to ensure progress")
        
        # Create chunks with a sliding window
        position = 0
        text_length = len(text)
        
        while position < text_length:
            # Calculate end position for current chunk
            end = min(position + self.chunk_size, text_length)
            
            # Extract the chunk
            chunk = text[position:end]
            
            # Only add non-empty chunks
            if chunk.strip():
                chunks.append(chunk)
            
            # Move position forward by step_size
            position += step_size
            
            # Safety check
            if position >= text_length:
                break
                
            # Progress indicator for large texts
            if text_length > 100000 and len(chunks) % 10 == 0:
                print(f"Chunking progress: {min(position, text_length)}/{text_length} characters")
        
        print(f"Created {len(chunks)} chunks from {text_length} characters of text")
        return chunks
    
    def chunk_by_separator(self, text: str, separator: str = "\n\n") -> List[str]:
        """
        Split text by separator first, then ensure chunks are within size limits.
        
        Args:
            text: The text to split
            separator: The separator to split on (default: paragraph breaks)
            
        Returns:
            List of text chunks
        """
        # Handle empty text
        if not text or not text.strip():
            return [""]
            
        # Handle short text
        if len(text) <= self.chunk_size:
            return [text]
        
        # Split by separator
        parts = text.split(separator)
        print(f"Split text into {len(parts)} parts using separator '{separator}'")
        
        # Filter out empty parts
        parts = [part for part in parts if part.strip()]
        
        # Handle case where there are no meaningful parts
        if not parts:
            return [""]
            
        # Handle case where each part is already small enough
        if all(len(part) <= self.chunk_size for part in parts):
            print("All parts are within chunk size limit")
            return parts
        
        # Combine parts into chunks that fit within chunk_size
        chunks = []
        current_chunk = ""
        
        for part in parts:
            # If this part alone exceeds chunk size, we need to split it further
            if len(part) > self.chunk_size:
                # First add any accumulated chunk
                if current_chunk:
                    chunks.append(current_chunk)
                    current_chunk = ""
                    
                # Then split the large part using the regular chunker
                part_chunks = self.chunk_text(part)
                chunks.extend(part_chunks)
                continue
                
            # If adding this part would exceed chunk size, start a new chunk
            if current_chunk and len(current_chunk) + len(separator) + len(part) > self.chunk_size:
                chunks.append(current_chunk)
                current_chunk = part
            # Otherwise add to current chunk
            else:
                if current_chunk:
                    current_chunk += separator + part
                else:
                    current_chunk = part
        
        # Add the last chunk if there is one
        if current_chunk:
            chunks.append(current_chunk)
            
        print(f"Created {len(chunks)} chunks using separator-based chunking")
        return chunks



================================================
FILE: foundational-rag-agent/document_processing/embeddings.py
================================================
"""
Embeddings generation with OpenAI for document processing.
"""
import os
import time
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import openai
from pathlib import Path

# Load environment variables from the project root .env file
project_root = Path(__file__).resolve().parent.parent
dotenv_path = project_root / '.env'

# Force override of existing environment variables
load_dotenv(dotenv_path, override=True)

class EmbeddingGenerator:
    """
    Simple and reliable embedding generator using OpenAI's API.
    """
    
    def __init__(self):
        """Initialize the embedding generator with API key from environment variables."""
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
        
        if not self.api_key:
            raise ValueError("OpenAI API key must be provided as OPENAI_API_KEY environment variable.")
        
        # Set up the OpenAI client
        self.client = openai.OpenAI(api_key=self.api_key)
        
        # Default embedding dimension for text-embedding-3-small
        self.embedding_dim = 1536
        
        print(f"Initialized EmbeddingGenerator with model: {self.model}")
    
    def _create_zero_embedding(self) -> List[float]:
        """Create a zero vector with the correct dimension."""
        return [0.0] * self.embedding_dim
    
    def embed_text(self, text: str, max_retries: int = 3) -> List[float]:
        """
        Generate an embedding for a single text with retry logic.
        
        Args:
            text: The text to embed
            max_retries: Maximum number of retry attempts
            
        Returns:
            Embedding vector
        """
        # Handle empty text
        if not text or not text.strip():
            print("Warning: Empty text provided, returning zero embedding")
            return self._create_zero_embedding()
        
        # Truncate very long text to avoid API limits
        max_length = 8000
        if len(text) > max_length:
            print(f"Warning: Text exceeds {max_length} characters, truncating")
            text = text[:max_length]
        
        # Try to generate embedding with retries
        for attempt in range(max_retries):
            try:
                response = self.client.embeddings.create(
                    model=self.model,
                    input=text
                )
                return response.data[0].embedding
            except Exception as e:
                print(f"Embedding error (attempt {attempt+1}/{max_retries}): {str(e)}")
                if attempt < max_retries - 1:
                    # Exponential backoff
                    time.sleep(2 ** attempt)
                else:
                    print("All retry attempts failed, returning zero embedding")
                    return self._create_zero_embedding()
    
    def embed_batch(self, texts: List[str], batch_size: int = 5) -> List[List[float]]:
        """
        Generate embeddings for multiple texts in small batches.
        
        Args:
            texts: List of texts to embed
            batch_size: Number of texts to process in each batch
            
        Returns:
            List of embedding vectors
        """
        # Filter out empty texts
        valid_texts = [text for text in texts if text and text.strip()]
        
        if not valid_texts:
            print("No valid texts to embed")
            return []
        
        results = []
        
        # Process in small batches to avoid memory issues
        for i in range(0, len(valid_texts), batch_size):
            batch = valid_texts[i:i+batch_size]
            print(f"Processing batch {i//batch_size + 1}/{(len(valid_texts)-1)//batch_size + 1} with {len(batch)} texts")
            
            # Process each text individually for better error isolation
            batch_results = []
            for text in batch:
                embedding = self.embed_text(text)
                batch_results.append(embedding)
            
            results.extend(batch_results)
            
            # Small delay between batches to avoid rate limiting
            if i + batch_size < len(valid_texts):
                time.sleep(0.5)
        
        print(f"Successfully embedded {len(results)} texts")
        return results



================================================
FILE: foundational-rag-agent/document_processing/ingestion.py
================================================
"""
Document ingestion pipeline for processing documents and generating embeddings.
"""
import os
import uuid
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
from datetime import datetime

from document_processing.chunker import TextChunker
from document_processing.embeddings import EmbeddingGenerator
from document_processing.processors import get_document_processor
from database.setup import SupabaseClient

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DocumentIngestionPipeline:
    """
    Simplified document ingestion pipeline with robust error handling.
    """
    
    def __init__(self, supabase_client: Optional[SupabaseClient] = None):
        """
        Initialize the document ingestion pipeline with default components.
        
        Args:
            supabase_client: Optional SupabaseClient for database operations
        """
        self.chunker = TextChunker(chunk_size=1000, chunk_overlap=200)
        self.embedding_generator = EmbeddingGenerator()
        self.max_file_size_mb = 10  # Maximum file size in MB
        self.supabase_client = supabase_client or SupabaseClient()
        
        logger.info("Initialized DocumentIngestionPipeline with default components")
    
    def _check_file(self, file_path: str) -> bool:
        """
        Validate file exists and is within size limits.
        
        Args:
            file_path: Path to the document file
            
        Returns:
            True if file is valid, False otherwise
        """
        # Check if file exists
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return False
            
        # Check file size
        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)
        if file_size_mb > self.max_file_size_mb:
            logger.error(f"File size ({file_size_mb:.2f} MB) exceeds maximum allowed size ({self.max_file_size_mb} MB)")
            return False
            
        return True
    
    def process_file(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Process a document file, extract text, generate chunks and embeddings.
        
        Args:
            file_path: Path to the document file
            metadata: Optional metadata to associate with the document
            
        Returns:
            List of document chunks with embeddings
        """
        # Validate file
        if not self._check_file(file_path):
            return []
        
        # Get appropriate document processor
        try:
            processor = get_document_processor(file_path)
            if not processor:
                logger.error(f"Unsupported file type: {file_path}")
                return []
        except Exception as e:
            logger.error(f"Error getting document processor: {str(e)}")
            return []
        
        # Extract text from document
        try:
            text = processor.extract_text(file_path)
            logger.info(f"Extracted {len(text)} characters from {os.path.basename(file_path)}")
            
            if not text or not text.strip():
                logger.warning(f"No text content extracted from {os.path.basename(file_path)}")
                return []
                
        except Exception as e:
            logger.error(f"Failed to extract text from {os.path.basename(file_path)}: {str(e)}")
            return []
        
        # Generate chunks
        try:
            chunks = self.chunker.chunk_text(text)
            
            # Filter out empty chunks
            chunks = [chunk for chunk in chunks if chunk and chunk.strip()]
            
            if not chunks:
                logger.warning("No valid chunks generated from document")
                return []
                
            logger.info(f"Generated {len(chunks)} valid chunks from document")
            
        except Exception as e:
            logger.error(f"Error chunking document: {str(e)}")
            return []
        
        # Generate embeddings for chunks
        try:
            embeddings = self.embedding_generator.embed_batch(chunks, batch_size=5)
            
            if len(embeddings) != len(chunks):
                logger.warning(f"Mismatch between chunks ({len(chunks)}) and embeddings ({len(embeddings)})")
                # Ensure we only process chunks that have embeddings
                chunks = chunks[:len(embeddings)]
                
            logger.info(f"Generated {len(embeddings)} embeddings")
            
        except Exception as e:
            logger.error(f"Error generating embeddings: {str(e)}")
            return []
        
        # Create document records
        try:
            # Generate a unique document ID
            document_id = str(uuid.uuid4())
            timestamp = datetime.now().isoformat()
            
            # Prepare metadata
            if metadata is None:
                metadata = {}
            
            # Add file info to metadata
            metadata.update({
                "filename": os.path.basename(file_path),
                "file_path": file_path,
                "file_size_bytes": os.path.getsize(file_path),
                "processed_at": timestamp,
                "chunk_count": len(chunks)
            })
            
            # Create records and store in database
            records = []
            stored_records = []
            
            # Create a URL/identifier for the document
            url = f"file://{os.path.basename(file_path)}"
            
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                # Create record for return value
                record = {
                    "id": f"{document_id}_{i}",
                    "document_id": document_id,
                    "chunk_index": i,
                    "text": chunk,
                    "embedding": embedding,
                    "metadata": metadata.copy()
                }
                records.append(record)
                
                # Store in Supabase
                try:
                    stored_record = self.supabase_client.store_document_chunk(
                        url=url,
                        chunk_number=i,
                        content=chunk,
                        embedding=embedding,
                        metadata=metadata.copy()
                    )
                    stored_records.append(stored_record)
                except Exception as e:
                    logger.error(f"Error storing chunk {i} in database: {str(e)}")
            
            logger.info(f"Created {len(records)} document records with ID {document_id}")
            logger.info(f"Stored {len(stored_records)} chunks in database")
            return records
            
        except Exception as e:
            logger.error(f"Error creating document records: {str(e)}")
            return []
    
    def process_text(self, text: str, source_id: str, metadata: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Process a text string through the ingestion pipeline.
        
        Args:
            text: Text content to process
            source_id: Identifier for the source of the text
            metadata: Optional metadata about the text
            
        Returns:
            List of dictionaries containing the processed chunks with their IDs
        """
        if not text or not text.strip():
            logger.warning("Empty text provided to process_text")
            return []
            
        if metadata is None:
            metadata = {}
        
        # Add source information to metadata
        metadata.update({
            "source_type": "text",
            "source_id": source_id,
            "processed_at": datetime.now().isoformat()
        })
        
        try:
            # Generate chunks
            chunks = self.chunker.chunk_text(text)
            chunks = [chunk for chunk in chunks if chunk and chunk.strip()]
            
            if not chunks:
                logger.warning("No valid chunks generated from text")
                return []
                
            logger.info(f"Generated {len(chunks)} chunks from text")
            
            # Generate embeddings
            embeddings = self.embedding_generator.embed_batch(chunks)
            
            # Create document records
            document_id = str(uuid.uuid4())
            
            # Create records and store in database
            records = []
            stored_records = []
            
            # Create a URL/identifier for the text
            url = f"text://{source_id}"
            
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                # Create record for return value
                record = {
                    "id": f"{document_id}_{i}",
                    "document_id": document_id,
                    "chunk_index": i,
                    "text": chunk,
                    "embedding": embedding,
                    "metadata": metadata.copy()
                }
                records.append(record)
                
                # Store in Supabase
                try:
                    stored_record = self.supabase_client.store_document_chunk(
                        url=url,
                        chunk_number=i,
                        content=chunk,
                        embedding=embedding,
                        metadata=metadata.copy()
                    )
                    stored_records.append(stored_record)
                except Exception as e:
                    logger.error(f"Error storing text chunk {i} in database: {str(e)}")
            
            logger.info(f"Created {len(records)} records from text input")
            logger.info(f"Stored {len(stored_records)} text chunks in database")
            return records
            
        except Exception as e:
            logger.error(f"Error processing text: {str(e)}")
            return []
    
    def process_batch(self, file_paths: List[str], metadata: Optional[Dict[str, Any]] = None) -> Dict[str, List[Dict[str, Any]]]:
        """
        Process a batch of files through the ingestion pipeline.
        
        Args:
            file_paths: List of paths to document files
            metadata: Optional shared metadata for all files
            
        Returns:
            Dictionary mapping file paths to their processed chunks
        """
        results = {}
        
        for file_path in file_paths:
            try:
                # Create file-specific metadata
                file_metadata = metadata.copy() if metadata else {}
                file_metadata["batch_processed"] = True
                
                # Process the file
                file_results = self.process_file(file_path, file_metadata)
                results[file_path] = file_results
                
                logger.info(f"Processed {file_path} with {len(file_results)} chunks")
                
            except Exception as e:
                logger.error(f"Error processing {file_path}: {str(e)}")
                results[file_path] = []
        
        return results



================================================
FILE: foundational-rag-agent/document_processing/processors.py
================================================
"""
Document processors for extracting text from various file types.
"""
import os
import logging
from typing import Dict, Any, Optional, List
from pathlib import Path
import PyPDF2

# Set up logging
logger = logging.getLogger(__name__)

class DocumentProcessor:
    """
    Base class for document processors.
    """
    
    def extract_text(self, file_path: str) -> str:
        """
        Extract text content from a document file.
        
        Args:
            file_path: Path to the document file
            
        Returns:
            Extracted text content as a string
        """
        raise NotImplementedError("Subclasses must implement extract_text method")
    
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        Extract metadata from a document file.
        
        Args:
            file_path: Path to the document file
            
        Returns:
            Dictionary containing document metadata
        """
        # Basic metadata common to all file types
        path = Path(file_path)
        return {
            "filename": path.name,
            "file_extension": path.suffix.lower(),
            "file_size_bytes": path.stat().st_size,
            "created_at": path.stat().st_ctime,
            "modified_at": path.stat().st_mtime
        }


class TxtProcessor(DocumentProcessor):
    """
    Processor for plain text files with robust error handling.
    """
    
    def extract_text(self, file_path: str) -> str:
        """
        Extract text from a TXT file with encoding fallbacks.
        
        Args:
            file_path: Path to the TXT file
            
        Returns:
            Extracted text content
        """
        path = Path(file_path)
        
        # Validate file exists
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Try different encodings if UTF-8 fails
        encodings = ["utf-8", "latin-1", "cp1252", "ascii"]
        content = ""
        
        for encoding in encodings:
            try:
                with open(file_path, "r", encoding=encoding) as file:
                    content = file.read()
                logger.info(f"Successfully read text file with {encoding} encoding")
                break
            except UnicodeDecodeError:
                logger.warning(f"Failed to decode with {encoding}, trying next encoding")
            except Exception as e:
                logger.error(f"Error reading file with {encoding}: {str(e)}")
                raise
        
        if not content:
            raise ValueError(f"Could not decode file with any of the attempted encodings")
            
        return content
    
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        Get metadata for a TXT file.
        
        Args:
            file_path: Path to the TXT file
            
        Returns:
            Dictionary containing document metadata
        """
        metadata = super().get_metadata(file_path)
        metadata["content_type"] = "text/plain"
        metadata["processor"] = "TxtProcessor"
        
        # Count lines and words
        try:
            text = self.extract_text(file_path)
            metadata["line_count"] = len(text.splitlines())
            metadata["word_count"] = len(text.split())
        except Exception:
            # Don't fail metadata collection if text extraction fails
            pass
            
        return metadata


class PdfProcessor(DocumentProcessor):
    """
    Processor for PDF files with improved text extraction.
    """
    
    def extract_text(self, file_path: str) -> str:
        """
        Extract text from a PDF file with page tracking.
        
        Args:
            file_path: Path to the PDF file
            
        Returns:
            Extracted text content
        """
        path = Path(file_path)
        
        # Validate file exists
        if not path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")
        
        try:
            with open(file_path, "rb") as file:
                reader = PyPDF2.PdfReader(file)
                
                # Extract text from all pages with page numbers
                content = []
                total_pages = len(reader.pages)
                
                for page_num in range(total_pages):
                    try:
                        page = reader.pages[page_num]
                        page_text = page.extract_text()
                        
                        # Add page marker and text
                        if page_text and page_text.strip():
                            content.append(f"[Page {page_num + 1} of {total_pages}]\n{page_text}\n")
                    except Exception as e:
                        logger.warning(f"Error extracting text from page {page_num + 1}: {str(e)}")
                        content.append(f"[Page {page_num + 1} of {total_pages} - Text extraction failed]\n")
                
                return "\n".join(content)
                
        except Exception as e:
            logger.error(f"Error processing PDF file: {str(e)}")
            raise
    
    def get_metadata(self, file_path: str) -> Dict[str, Any]:
        """
        Get metadata for a PDF file including PDF-specific properties.
        
        Args:
            file_path: Path to the PDF file
            
        Returns:
            Dictionary containing document metadata
        """
        metadata = super().get_metadata(file_path)
        metadata["content_type"] = "application/pdf"
        metadata["processor"] = "PdfProcessor"
        
        # Extract PDF-specific metadata
        try:
            with open(file_path, "rb") as file:
                reader = PyPDF2.PdfReader(file)
                
                # Basic PDF properties
                metadata["page_count"] = len(reader.pages)
                
                # PDF document info if available
                if reader.metadata:
                    pdf_info = reader.metadata
                    if pdf_info.title:
                        metadata["title"] = pdf_info.title
                    if pdf_info.author:
                        metadata["author"] = pdf_info.author
                    if pdf_info.subject:
                        metadata["subject"] = pdf_info.subject
                    if pdf_info.creator:
                        metadata["creator"] = pdf_info.creator
                    if pdf_info.producer:
                        metadata["producer"] = pdf_info.producer
        except Exception as e:
            logger.warning(f"Error extracting PDF metadata: {str(e)}")
            
        return metadata


def get_document_processor(file_path: str) -> Optional[DocumentProcessor]:
    """
    Get the appropriate processor for a file based on its extension.
    
    Args:
        file_path: Path to the document file
        
    Returns:
        DocumentProcessor instance for the file type or None if unsupported
    """
    path = Path(file_path)
    extension = path.suffix.lower()
    
    processors = {
        ".txt": TxtProcessor(),
        ".pdf": PdfProcessor(),
        # Add more processors here as needed
    }
    
    processor = processors.get(extension)
    
    if processor:
        logger.info(f"Using {processor.__class__.__name__} for {path.name}")
        return processor
    else:
        logger.warning(f"Unsupported file type: {extension}")
        return None


def get_supported_extensions() -> List[str]:
    """
    Get a list of supported file extensions.
    
    Returns:
        List of supported file extensions
    """
    return [".txt", ".pdf"]



================================================
FILE: foundational-rag-agent/tests/test_agent.py
================================================
"""
Unit tests for the RAG agent module.
"""
import os
import sys
import pytest
from unittest.mock import MagicMock, patch
from typing import List, Dict, Any

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent.agent import RAGAgent
from agent.tools import KnowledgeBaseSearchResult


class TestRAGAgent:
    """
    Test cases for the RAGAgent class.
    """
    
    @pytest.mark.asyncio
    async def test_query_with_results(self):
        """
        Test that the agent query returns results when documents are found.
        """
        # Mock the Pydantic AI Agent and KnowledgeBaseSearch
        mock_agent = MagicMock()
        mock_kb_search = MagicMock()
        
        # Set up mock return values for the agent
        mock_result = MagicMock()
        mock_result.output = "This is the agent's response based on the knowledge base."
        mock_result.tool_calls = [MagicMock()]
        mock_result.tool_calls[0].tool.name = "search"
        
        # Create mock search results
        mock_search_results = [
            KnowledgeBaseSearchResult(
                content="This is test content 1.",
                source="test1.txt",
                source_type="txt",
                similarity=0.95,
                metadata={"source": "test1.txt", "source_type": "txt"}
            ),
            KnowledgeBaseSearchResult(
                content="This is test content 2.",
                source="test2.txt",
                source_type="txt",
                similarity=0.85,
                metadata={"source": "test2.txt", "source_type": "txt"}
            )
        ]
        mock_result.tool_calls[0].result = mock_search_results
        
        # Set the agent's run method to return the mock result
        mock_agent.run.return_value = mock_result
        
        # Create the RAGAgent with mocks
        with patch('agent.agent.Agent', return_value=mock_agent):
            rag_agent = RAGAgent(
                model="gpt-4.1-mini",
                api_key="test_api_key",
                kb_search=mock_kb_search
            )
            
            # Call the query method
            result = await rag_agent.query("What is the test content?")
            
            # Check that the agent was called correctly
            mock_agent.run.assert_called_once()
            
            # Check the result
            assert result["response"] == "This is the agent's response based on the knowledge base."
            assert len(result["kb_results"]) == 2
            assert result["kb_results"][0].content == "This is test content 1."
            assert result["kb_results"][1].content == "This is test content 2."
    
    @pytest.mark.asyncio
    async def test_query_no_kb_results(self):
        """
        Test that the agent query works when no knowledge base results are found.
        """
        # Mock the Pydantic AI Agent and KnowledgeBaseSearch
        mock_agent = MagicMock()
        mock_kb_search = MagicMock()
        
        # Set up mock return values for the agent
        mock_result = MagicMock()
        mock_result.output = "I don't have specific information about that in my knowledge base."
        mock_result.tool_calls = [MagicMock()]
        mock_result.tool_calls[0].tool.name = "search"
        mock_result.tool_calls[0].result = []  # Empty results
        
        # Set the agent's run method to return the mock result
        mock_agent.run.return_value = mock_result
        
        # Create the RAGAgent with mocks
        with patch('agent.agent.Agent', return_value=mock_agent):
            rag_agent = RAGAgent(
                model="gpt-4.1-mini",
                api_key="test_api_key",
                kb_search=mock_kb_search
            )
            
            # Call the query method
            result = await rag_agent.query("What is something not in the knowledge base?")
            
            # Check that the agent was called correctly
            mock_agent.run.assert_called_once()
            
            # Check the result
            assert result["response"] == "I don't have specific information about that in my knowledge base."
            assert len(result["kb_results"]) == 0
    
    @pytest.mark.asyncio
    async def test_get_available_sources(self):
        """
        Test that get_available_sources returns the list of sources.
        """
        # Mock the KnowledgeBaseSearch
        mock_kb_search = MagicMock()
        
        # Set up mock return values
        mock_sources = ["test1.txt", "test2.pdf", "test3.txt"]
        mock_kb_search.get_available_sources.return_value = mock_sources
        
        # Create the RAGAgent with mock
        with patch('agent.agent.Agent'):
            rag_agent = RAGAgent(
                model="gpt-4.1-mini",
                api_key="test_api_key",
                kb_search=mock_kb_search
            )
            
            # Call the get_available_sources method
            sources = await rag_agent.get_available_sources()
            
            # Check that the mock was called correctly
            mock_kb_search.get_available_sources.assert_called_once()
            
            # Check the results
            assert sources == mock_sources



================================================
FILE: foundational-rag-agent/tests/test_agent_tools.py
================================================
"""
Unit tests for the agent tools module.
"""
import os
import sys
import pytest
import asyncio
from unittest.mock import MagicMock, patch
from typing import List, Dict, Any

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent.tools import KnowledgeBaseSearch, KnowledgeBaseSearchParams, KnowledgeBaseSearchResult


class TestKnowledgeBaseSearch:
    """
    Test cases for the KnowledgeBaseSearch class.
    """
    
    @pytest.mark.asyncio
    async def test_search_with_results(self):
        """
        Test that search returns results when documents are found.
        """
        # Mock the SupabaseClient and EmbeddingGenerator
        mock_supabase = MagicMock()
        mock_embedding_generator = MagicMock()
        
        # Set up mock return values
        mock_embedding = [0.1] * 1536  # Mock embedding vector
        mock_embedding_generator.embed_text.return_value = mock_embedding
        
        mock_search_results = [
            {
                "id": 1,
                "url": "local://test1.txt",
                "chunk_number": 0,
                "content": "This is test content 1.",
                "metadata": {
                    "source": "test1.txt",
                    "source_type": "txt"
                },
                "similarity": 0.95
            },
            {
                "id": 2,
                "url": "local://test2.txt",
                "chunk_number": 1,
                "content": "This is test content 2.",
                "metadata": {
                    "source": "test2.txt",
                    "source_type": "txt"
                },
                "similarity": 0.85
            }
        ]
        mock_supabase.search_documents.return_value = mock_search_results
        
        # Create the KnowledgeBaseSearch instance with mocks
        kb_search = KnowledgeBaseSearch(
            supabase_client=mock_supabase,
            embedding_generator=mock_embedding_generator
        )
        
        # Create search parameters
        params = KnowledgeBaseSearchParams(
            query="test query",
            max_results=2
        )
        
        # Call the search method
        results = await kb_search.search(params)
        
        # Check that the mocks were called correctly
        mock_embedding_generator.embed_text.assert_called_once_with("test query")
        mock_supabase.search_documents.assert_called_once_with(
            query_embedding=mock_embedding,
            match_count=2,
            filter_metadata=None
        )
        
        # Check the results
        assert len(results) == 2
        assert isinstance(results[0], KnowledgeBaseSearchResult)
        assert results[0].content == "This is test content 1."
        assert results[0].source == "test1.txt"
        assert results[0].source_type == "txt"
        assert results[0].similarity == 0.95
        
        assert results[1].content == "This is test content 2."
        assert results[1].source == "test2.txt"
        assert results[1].source_type == "txt"
        assert results[1].similarity == 0.85
    
    @pytest.mark.asyncio
    async def test_search_with_source_filter(self):
        """
        Test that search applies source filter correctly.
        """
        # Mock the SupabaseClient and EmbeddingGenerator
        mock_supabase = MagicMock()
        mock_embedding_generator = MagicMock()
        
        # Set up mock return values
        mock_embedding = [0.1] * 1536  # Mock embedding vector
        mock_embedding_generator.embed_text.return_value = mock_embedding
        
        mock_search_results = [
            {
                "id": 1,
                "url": "local://test1.txt",
                "chunk_number": 0,
                "content": "This is test content 1.",
                "metadata": {
                    "source": "test1.txt",
                    "source_type": "txt"
                },
                "similarity": 0.95
            }
        ]
        mock_supabase.search_documents.return_value = mock_search_results
        
        # Create the KnowledgeBaseSearch instance with mocks
        kb_search = KnowledgeBaseSearch(
            supabase_client=mock_supabase,
            embedding_generator=mock_embedding_generator
        )
        
        # Create search parameters with source filter
        params = KnowledgeBaseSearchParams(
            query="test query",
            max_results=5,
            source_filter="test1.txt"
        )
        
        # Call the search method
        results = await kb_search.search(params)
        
        # Check that the mocks were called correctly
        mock_embedding_generator.embed_text.assert_called_once_with("test query")
        mock_supabase.search_documents.assert_called_once_with(
            query_embedding=mock_embedding,
            match_count=5,
            filter_metadata={"source": "test1.txt"}
        )
        
        # Check the results
        assert len(results) == 1
        assert results[0].source == "test1.txt"
    
    @pytest.mark.asyncio
    async def test_search_no_results(self):
        """
        Test that search returns empty list when no documents are found.
        """
        # Mock the SupabaseClient and EmbeddingGenerator
        mock_supabase = MagicMock()
        mock_embedding_generator = MagicMock()
        
        # Set up mock return values
        mock_embedding = [0.1] * 1536  # Mock embedding vector
        mock_embedding_generator.embed_text.return_value = mock_embedding
        
        # Return empty results
        mock_supabase.search_documents.return_value = []
        
        # Create the KnowledgeBaseSearch instance with mocks
        kb_search = KnowledgeBaseSearch(
            supabase_client=mock_supabase,
            embedding_generator=mock_embedding_generator
        )
        
        # Create search parameters
        params = KnowledgeBaseSearchParams(
            query="test query",
            max_results=5
        )
        
        # Call the search method
        results = await kb_search.search(params)
        
        # Check the results
        assert len(results) == 0
    
    @pytest.mark.asyncio
    async def test_get_available_sources(self):
        """
        Test that get_available_sources returns the list of sources.
        """
        # Mock the SupabaseClient
        mock_supabase = MagicMock()
        
        # Set up mock return values
        mock_sources = ["test1.txt", "test2.pdf", "test3.txt"]
        mock_supabase.get_all_document_sources.return_value = mock_sources
        
        # Create the KnowledgeBaseSearch instance with mock
        kb_search = KnowledgeBaseSearch(supabase_client=mock_supabase)
        
        # Call the get_available_sources method
        sources = await kb_search.get_available_sources()
        
        # Check that the mock was called correctly
        mock_supabase.get_all_document_sources.assert_called_once()
        
        # Check the results
        assert sources == mock_sources



================================================
FILE: foundational-rag-agent/tests/test_chunker.py
================================================
"""
Unit tests for the text chunker module.
"""
import os
import sys
import pytest
from pathlib import Path

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from document_processing.chunker import TextChunker


class TestTextChunker:
    """
    Test cases for the TextChunker class.
    """
    
    def test_init_with_default_values(self):
        """
        Test that TextChunker initializes with default values.
        """
        chunker = TextChunker()
        assert chunker.chunk_size == 1000
        assert chunker.chunk_overlap == 200
    
    def test_init_with_custom_values(self):
        """
        Test that TextChunker initializes with custom values.
        """
        chunker = TextChunker(chunk_size=500, chunk_overlap=100)
        assert chunker.chunk_size == 500
        assert chunker.chunk_overlap == 100
    
    def test_init_with_large_overlap(self):
        """
        Test that TextChunker adjusts overlap when it's too large.
        """
        # In our new implementation, we automatically adjust the overlap
        # to be at most half of the chunk size
        chunker = TextChunker(chunk_size=500, chunk_overlap=500)
        assert chunker.chunk_overlap == 250  # Should be adjusted to chunk_size // 2
        
        chunker = TextChunker(chunk_size=500, chunk_overlap=600)
        assert chunker.chunk_overlap == 250  # Should be adjusted to chunk_size // 2
    
    def test_chunk_text_short_text(self):
        """
        Test chunking text that is shorter than chunk_size.
        """
        chunker = TextChunker(chunk_size=1000, chunk_overlap=200)
        text = "This is a short text."
        chunks = chunker.chunk_text(text)
        
        assert len(chunks) == 1
        assert chunks[0] == text
    
    def test_chunk_text_long_text(self):
        """
        Test chunking text that is longer than chunk_size.
        """
        chunker = TextChunker(chunk_size=100, chunk_overlap=20)
        text = "This is a longer text that should be split into multiple chunks. " * 5
        chunks = chunker.chunk_text(text)
        
        assert len(chunks) > 1
        
        # Check that the chunks cover the entire text
        reconstructed = ""
        for i, chunk in enumerate(chunks):
            if i == 0:
                reconstructed += chunk
            else:
                # Account for overlap
                overlap_start = len(reconstructed) - chunker.chunk_overlap
                if overlap_start > 0:
                    reconstructed = reconstructed[:overlap_start] + chunk
                else:
                    reconstructed += chunk
        
        # The reconstructed text might be slightly different due to splitting at sentence boundaries
        assert len(reconstructed) >= len(text) * 0.9
    
    def test_chunk_by_separator(self):
        """
        Test splitting text by a separator.
        """
        chunker = TextChunker(chunk_size=100, chunk_overlap=20)
        paragraphs = [
            "This is the first paragraph.",
            "This is the second paragraph.",
            "This is the third paragraph.",
            "This is the fourth paragraph."
        ]
        text = "\n\n".join(paragraphs)
        
        chunks = chunker.chunk_by_separator(text, separator="\n\n")
        
        assert len(chunks) == 4
        for i, paragraph in enumerate(paragraphs):
            assert paragraph in chunks[i]
    
    def test_chunk_by_separator_large_paragraph(self):
        """
        Test splitting text by a separator with a paragraph larger than chunk_size.
        """
        chunker = TextChunker(chunk_size=50, chunk_overlap=10)
        paragraphs = [
            "This is a short paragraph.",
            "This is a very long paragraph that exceeds the chunk size and should be split into multiple chunks.",
            "This is another short paragraph."
        ]
        text = "\n\n".join(paragraphs)
        
        chunks = chunker.chunk_by_separator(text, separator="\n\n")
        
        assert len(chunks) > 3  # The long paragraph should be split
        assert paragraphs[0] in chunks[0]
        assert paragraphs[2] in chunks[-1]



================================================
FILE: foundational-rag-agent/tests/test_processors.py
================================================
"""
Unit tests for the document processors module.
"""
import os
import sys
import pytest
from pathlib import Path
import tempfile

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from document_processing.processors import DocumentProcessor, TxtProcessor, PdfProcessor, get_document_processor


class TestDocumentProcessor:
    """
    Test cases for the DocumentProcessor base class.
    """
    
    def test_extract_text_not_implemented(self):
        """
        Test that the base DocumentProcessor raises NotImplementedError for extract_text.
        """
        processor = DocumentProcessor()
        with pytest.raises(NotImplementedError):
            processor.extract_text("dummy_path")
            
    def test_get_metadata_basic(self):
        """
        Test that the base DocumentProcessor provides basic metadata.
        """
        # Create a temporary file for testing
        with tempfile.NamedTemporaryFile(suffix=".txt", delete=False) as temp_file:
            temp_file.write(b"Test content")
            temp_file_path = temp_file.name
            
        try:
            processor = DocumentProcessor()
            metadata = processor.get_metadata(temp_file_path)
            
            # Check basic metadata fields
            assert "filename" in metadata
            assert "file_extension" in metadata
            assert "file_size_bytes" in metadata
            assert metadata["file_extension"] == ".txt"
        finally:
            # Clean up
            os.unlink(temp_file_path)


class TestTxtProcessor:
    """
    Test cases for the TxtProcessor class.
    """
    
    def test_extract_text_nonexistent_file(self):
        """
        Test that TxtProcessor raises FileNotFoundError for nonexistent files.
        """
        processor = TxtProcessor()
        with pytest.raises(FileNotFoundError):
            processor.extract_text("nonexistent_file.txt")
    
    def test_extract_text_valid_txt_file(self):
        """
        Test that TxtProcessor correctly extracts text from a valid TXT file.
        """
        # Create a temporary TXT file
        content = "This is a test document.\nIt has multiple lines.\nAnd some content to process."
        with tempfile.NamedTemporaryFile(suffix=".txt", delete=False) as temp_file:
            temp_file.write(content.encode("utf-8"))
            temp_file_path = temp_file.name
        
        try:
            processor = TxtProcessor()
            extracted_text = processor.extract_text(temp_file_path)
            
            # Check the extracted text
            assert extracted_text == content
        finally:
            # Clean up the temporary file
            os.unlink(temp_file_path)
    
    def test_get_metadata_txt_file(self):
        """
        Test that TxtProcessor correctly extracts metadata from a TXT file.
        """
        # Create a temporary TXT file
        content = "This is a test document.\nIt has multiple lines.\nAnd some content to process."
        with tempfile.NamedTemporaryFile(suffix=".txt", delete=False) as temp_file:
            temp_file.write(content.encode("utf-8"))
            temp_file_path = temp_file.name
        
        try:
            processor = TxtProcessor()
            metadata = processor.get_metadata(temp_file_path)
            
            # Check metadata fields
            assert "filename" in metadata
            assert "file_extension" in metadata
            assert "file_size_bytes" in metadata
            assert "content_type" in metadata
            assert metadata["file_extension"] == ".txt"
            assert metadata["content_type"] == "text/plain"
            assert "processor" in metadata
            assert metadata["processor"] == "TxtProcessor"
        finally:
            # Clean up the temporary file
            os.unlink(temp_file_path)


class TestPdfProcessor:
    """
    Test cases for the PdfProcessor class.
    """
    
    def test_extract_text_nonexistent_file(self):
        """
        Test that PdfProcessor raises FileNotFoundError for nonexistent files.
        """
        processor = PdfProcessor()
        with pytest.raises(FileNotFoundError):
            processor.extract_text("nonexistent_file.pdf")
    
    def test_get_metadata_pdf_file(self):
        """
        Test that PdfProcessor correctly extracts metadata.
        Note: This test doesn't use a real PDF file to avoid dependencies,
        but just tests the error handling.
        """
        processor = PdfProcessor()
        
        # Since we can't easily create a valid PDF file in a test,
        # we'll just test that the method handles errors gracefully
        with pytest.raises(FileNotFoundError):
            processor.get_metadata("nonexistent_file.pdf")


class TestGetDocumentProcessor:
    """
    Test cases for the get_document_processor function.
    """
    
    def test_get_document_processor_txt(self):
        """
        Test that get_document_processor returns a TxtProcessor for .txt files.
        """
        processor = get_document_processor("test.txt")
        assert isinstance(processor, TxtProcessor)
    
    def test_get_document_processor_pdf(self):
        """
        Test that get_document_processor returns a PdfProcessor for .pdf files.
        """
        processor = get_document_processor("test.pdf")
        assert isinstance(processor, PdfProcessor)
    
    def test_get_document_processor_unsupported(self):
        """
        Test that get_document_processor returns None for unsupported file types.
        """
        assert get_document_processor("test.docx") is None
        assert get_document_processor("test.csv") is None



================================================
FILE: foundational-rag-agent/ui/app.py
================================================
"""
Streamlit application for the RAG AI agent.
"""
import os
import sys
import asyncio
from typing import List, Dict, Any
import streamlit as st
from pathlib import Path
import tempfile
from datetime import datetime

# Add parent directory to path to allow relative imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from document_processing.ingestion import DocumentIngestionPipeline
from document_processing.chunker import TextChunker
from document_processing.embeddings import EmbeddingGenerator
from database.setup import SupabaseClient
from agent.agent import RAGAgent, agent as rag_agent
from pydantic_ai.messages import ModelRequest, ModelResponse, PartDeltaEvent, PartStartEvent, TextPartDelta

# Set page configuration
st.set_page_config(
    page_title="RAG AI Agent",
    page_icon="🔍",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialize database client
supabase_client = SupabaseClient()

# Initialize session state
if "messages" not in st.session_state:
    st.session_state.messages = []
    
if "sources" not in st.session_state:
    st.session_state.sources = []
    
if "document_count" not in st.session_state:
    # Initialize document count from database
    try:
        st.session_state.document_count = supabase_client.count_documents()
    except Exception as e:
        print(f"Error getting document count: {e}")
        st.session_state.document_count = 0
    
if "processed_files" not in st.session_state:
    st.session_state.processed_files = set()  # Track already processed files


def display_message_part(part):
    """
    Display a single part of a message in the Streamlit UI.
    
    Args:
        part: Message part to display
    """
    # User messages
    if part.part_kind == 'user-prompt' and part.content:
        with st.chat_message("user"):
            st.markdown(part.content)
    # AI messages
    elif part.part_kind == 'text' and part.content:
        with st.chat_message("assistant"):
            st.markdown(part.content)


async def process_document(file_path: str) -> Dict[str, Any]:
    """
    Process a document file and store it in the knowledge base.
    
    Args:
        file_path: Path to the document file
        
    Returns:
        Dictionary containing information about the processed document
    """
    # Create document ingestion pipeline with default settings
    # The pipeline now handles chunking and embedding internally
    pipeline = DocumentIngestionPipeline()
    
    # Process the file
    try:
        # Add file-specific metadata
        metadata = {
            "source": "ui_upload",
            "upload_time": str(datetime.now())
        }
        
        # Use asyncio to run the CPU-bound processing in a thread pool
        # This prevents blocking the Streamlit UI thread
        loop = asyncio.get_event_loop()
        
        # Process the file in a non-blocking way
        # Using a lambda to properly handle instance methods
        chunks = await loop.run_in_executor(
            None,  # Use default executor
            lambda: pipeline.process_file(file_path, metadata)
        )
        
        if not chunks:
            return {
                "success": False,
                "file_path": file_path,
                "error": "No valid chunks were generated from the document"
            }
        
        return {
            "success": True,
            "file_path": file_path,
            "chunk_count": len(chunks)
        }
    except Exception as e:
        import traceback
        print(f"Error processing document: {str(e)}")
        print(traceback.format_exc())
        return {
            "success": False,
            "file_path": file_path,
            "error": str(e)
        }


async def run_agent_with_streaming(user_input: str):
    """
    Run the RAG agent with streaming response.
    
    Args:
        user_input: User query
        
    Yields:
        Streamed response chunks
    """
    # Run the agent with the user input
    async with rag_agent.agent.iter(user_input, deps={"kb_search": rag_agent.kb_search}, message_history=st.session_state.messages) as run:
        async for node in run:
            # Check if this is a model request node
            if hasattr(node, 'request') and isinstance(node.request, ModelRequest):
                # Stream tokens from the model's request
                async with node.stream(run.ctx) as request_stream:
                    async for event in request_stream:
                        if isinstance(event, PartStartEvent) and event.part.part_kind == 'text':
                            yield event.part.content
                        elif isinstance(event, PartDeltaEvent) and isinstance(event.delta, TextPartDelta):
                            delta = event.delta.content_delta
                            yield delta
    
    # Add the new messages to the chat history
    st.session_state.messages.extend(run.result.new_messages())


async def update_available_sources():
    """
    Update the list of available sources in the knowledge base and refresh document count.
    """
    # Update sources list
    sources = await rag_agent.get_available_sources()
    st.session_state.sources = sources
    
    # Refresh document count from database
    try:
        st.session_state.document_count = supabase_client.count_documents()
    except Exception as e:
        print(f"Error updating document count: {e}")


async def main():
    """
    Main function for the Streamlit application.
    """
    # Display header
    st.title("🔍 RAG AI Agent")
    st.markdown(
        """
        This application allows you to upload documents (TXT and PDF) to a knowledge base 
        and ask questions that will be answered using the information in those documents.
        """
    )
    
    # Sidebar for document upload
    with st.sidebar:
        st.header("📄 Document Upload")
        
        # File uploader
        uploaded_files = st.file_uploader(
            "Upload documents to the knowledge base",
            type=["txt", "pdf"],
            accept_multiple_files=True
        )
        
        # Process only new uploaded files
        if uploaded_files:
            # Get list of files that haven't been processed yet
            new_files = []
            for uploaded_file in uploaded_files:
                # Create a unique identifier for the file based on name and content hash
                file_id = f"{uploaded_file.name}_{hash(uploaded_file.getvalue().hex())}"
                
                # Check if this file has already been processed
                if file_id not in st.session_state.processed_files:
                    new_files.append((uploaded_file, file_id))
            
            # Only show progress bar if there are new files to process
            if new_files:
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                total_files = len(new_files)
                for i, (uploaded_file, file_id) in enumerate(new_files):
                    # Update progress
                    progress = (i / total_files)
                    progress_bar.progress(progress)
                    status_text.text(f"Processing {uploaded_file.name}... ({i+1}/{total_files})")
                    
                    # Create a temporary file
                    with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as temp_file:
                        temp_file.write(uploaded_file.getvalue())
                        temp_file_path = temp_file.name
                    
                    try:
                        # Process the document
                        result = await process_document(temp_file_path)
                        
                        # Display result
                        if result["success"]:
                            st.success(f"Processed {uploaded_file.name}: {result['chunk_count']} chunks")
                            st.session_state.document_count += 1
                            # Mark this file as processed
                            st.session_state.processed_files.add(file_id)
                        else:
                            st.error(f"Error processing {uploaded_file.name}: {result['error']}")
                    finally:
                        # Remove temporary file
                        os.unlink(temp_file_path)
                
                # Complete progress bar
                progress_bar.progress(1.0)
                status_text.text("All documents processed!")
                
                # Update available sources
                await update_available_sources()
            elif uploaded_files:  # If we have files but none are new
                st.info("All files have already been processed.")
        
        # Display document count
        st.metric("Documents in Knowledge Base", st.session_state.document_count)
        
        # Display available sources
        if st.session_state.sources:
            st.subheader("Available Sources")
            for source in st.session_state.sources:
                st.write(f"- {source}")
    
    # Main chat interface
    st.header("💬 Chat with the AI")
    
    # Display all messages from the conversation so far
    for msg in st.session_state.messages:
        if isinstance(msg, ModelRequest) or isinstance(msg, ModelResponse):
            for part in msg.parts:
                display_message_part(part)
    
    # Chat input
    if user_input := st.chat_input("Ask a question about your documents..."):
        # Display user message
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # Display assistant response with streaming
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            
            # Stream the response
            generator = run_agent_with_streaming(user_input)
            async for chunk in generator:
                full_response += chunk
                message_placeholder.markdown(full_response + "▌")
            
            # Final response without cursor
            message_placeholder.markdown(full_response)


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: foundational-rag-agent/.windsurf/rules/primary-guide.md
================================================
---
trigger: always_on
---

### Crawl4AI MCP
- **Always use the Crawl4AI MCP server** to reference documentation for libraries like Pydantic AI and Mem0.
- For the tokens, always use 5000 tokens for your search.
- **Only search three times maximum for any specific piece of documentation.** If you don't get what you need, use the Brave MCP server to perform a wider search.

### 🔄 Project Awareness & Context
- **Always read `PLANNING.md`** at the start of a new conversation to understand the project's architecture, goals, style, and constraints.
- **Check `TASK.md`** before starting a new task. If the task isn’t listed, add it with a brief description and today's date.
- **Use consistent naming conventions, file structure, and architecture patterns** as described in `PLANNING.md`.
 
### 🧱 Code Structure & Modularity
- **Never create a file longer than 500 lines of code.** If a file approaches this limit, refactor by splitting it into modules or helper files.
- **Organize code into clearly separated modules**, grouped by feature or responsibility.
  For agents this looks like:
    - `agent.py` - Main agent definition and execution logic 
    - `tools.py` - Tool functions used by the agent 
    - `prompts.py` - System prompts
- **Use clear, consistent imports** (prefer relative imports within packages).

### 🧪 Testing & Reliability
- **Always create Pytest unit tests for new features** (functions, classes, routes, etc).
- **After updating any logic**, check whether existing unit tests need to be updated. If so, do it.
- **Tests should live in a `/tests` folder** mirroring the main app structure.
  - Include at least:
    - 1 test for expected use
    - 1 edge case
    - 1 failure case
- Always test the individual functions for agent tools.

### ✅ Task Completion
- **Mark completed tasks in `TASK.md`** immediately after finishing them.
- Add new sub-tasks or TODOs discovered during development to `TASK.md` under a “Discovered During Work” section.

### 📎 Style & Conventions
- **Use Python** as the primary language.
- **Follow PEP8**, use type hints, and format with `black`.
- **Use `pydantic` for data validation**.
- Don't use relative imports with "." or "..". Instead add on to the system path the directories you need to import from.
- Write **docstrings for every function** using the Google style:
  ```python
  def example():
      """
      Brief summary.

      Args:
          param1 (type): Description.

      Returns:
          type: Description.
      """
  ```

### 📚 Documentation & Explainability
- **Update `README.md`** when new features are added, dependencies change, or setup steps are modified.
- **Comment non-obvious code** and ensure everything is understandable to a mid-level developer.
- When writing complex logic, **add an inline `# Reason:` comment** explaining the why, not just the what.

### 🧠 AI Behavior Rules
- **Never assume missing context. Ask questions if uncertain.**
- **Always confirm file paths & module names** exist before using
- **Never delete or overwrite existing code** unless explicitly instructed to or if part of a task from `TASK.md`.

