# Manual Crawl â€“ Quickâ€‘Start Guide

> **Goal:** run `manual_crawl.py` inside the *crawl4aiâ€‘ragâ€‘mcp* Docker container and push fresh chunks into the `crawled_pages` table in Supabase.
>
> These steps assume you already have:
>
> * The project repo with its `Dockerfile` and `.env`.
> * Supabase CLI stack running on `http://localhost:54321`.
> * `crawl4ai-rag-mcp:latest` image built (or ready to build).
>
> Follow every step **in order**â€”skip nothing.

---

## 1Â Â Build (or rebuild) the image

```bash
# from the repo root
docker build -t crawl4ai-rag-mcp:latest .
```

If the build fails, check your Docker disk space (see *Troubleshooting* at the bottom).

---

## 2Â Â Start the container

```bash
# stop/remove any previous instance (safe if none exist)
docker stop  crawl4ai-rag-mcp-container || true
docker rm    crawl4ai-rag-mcp-container || true

# run the server + crawler tooling, publishing port 8051
# adjust paths/ports only if you know why
docker run -d \
  --name crawl4ai-rag-mcp-container \
  --env-file .env \
  -p 8051:8051 \
  crawl4ai-rag-mcp:latest
```

âœ…Â **Verify:**

```bash
docker ps --filter "name=crawl4ai-rag-mcp-container"
```

Should show `0.0.0.0:8051->8051/tcp` and a recent *Up* status.

---

## 3Â Â Open a shell inside the container

```bash
docker exec -it crawl4ai-rag-mcp-container bash
```

The prompt should change to `root@<container-id>:/app#`.

---

## 4Â Â Run `manual_crawl.py`

Basic syntax:

```bash
python src/manual_crawl.py \
  --url <START_URL> \
  [--max-depth N] \
  [--chunk-size BYTES] \
  [--batch-size M]
```

Where:

* **`--url`** *(required)* â€“ page or site to crawl.
* **`--max-depth`** â€“ how many linkâ€‘levels deep to follow (default `3`).

  * `1` â‡’ crawl only the start URL.
* **`--chunk-size`** â€“ split text into chunks of this size (default `5000`).
* **`--batch-size`** â€“ how many chunks to upsert per Supabase request (default `10`).

### 4.1Â Â Variants & examples

| Useâ€‘case                                      | Command                                                                                                          |
| --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
| **Single HTML page** (force exactly one page) | `python src/manual_crawl.py --url https://example.com/post/123 --max-depth 1`                                    |
| **Plain text file** (`llms-full.txt`)         | `python src/manual_crawl.py --url https://modelcontextprotocol.io/llms-full.txt --max-depth 1 --chunk-size 8000` |
| **Full site crawl** (default recursion)       | `python src/manual_crawl.py --url https://www.anthropic.com --chunk-size 4000`                                   |
| **Cap depth to 2 levels**                     | `python src/manual_crawl.py --url https://docs.python.org --max-depth 2`                                         |

> **Tip:** depth `N` crawls pages that are **â‰¤â€¯Nâ€“1 clicks** away from the start URL.

### 4.2Â Â What success looks like

Youâ€™ll see three phases per URL:

```
[FETCH]... â†“ <url> | âœ“ | â±: 1.23s
[SCRAPE].. â—† <url> | âœ“ | â±: 0.04s
[COMPLETE] â— <url> | âœ“ | â±: 1.27s
```

Then chunking progress bars, OpenAI embedding calls, and a final JSON summary:

```json
{"success": true, "pages_crawled": 12, "chunks_stored": 98}
```

Leave the container shell:

```bash
exit
```

---

## 5Â Â Confirm rows in Supabase

Run in any psql window or Supabase Studio:

```sql
SELECT url, chunk_number, metadata->>'chunk_index' AS chunk
FROM crawled_pages
ORDER BY id DESC
LIMIT 10;
```

You should see your recent crawl at the top.

---

## Troubleshooting

| Symptom                                                                          | Fix                                                                                                               |
| -------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| **`FATAL: could not write lock file "postmaster.pid": No space left on device`** | Free Docker disk space with `docker system prune -af` *or* enlarge Docker Desktopâ€™s disk image.                   |
| ImportError: `No module named crawler`                                           | Make sure youâ€™re inside the container **and** running `python src/manual_crawl.py` (not `python -m crawler.cli`). |
| Supabase 400 â€œmissing column chunk\_numberâ€                                      | Column is present in new schema; doubleâ€‘check you ran the latest migration.                                       |

---

## Recap

1. **Build** image â†’ 2. **Run** container â†’ 3. **Exec** shell â†’ 4. **Call** `manual_crawl.py` with the right flags â†’ 5. **Verify** rows.

Thatâ€™s all you need to keep your `crawled_pages` shelf stocked for hybrid RAG ğŸ‰
